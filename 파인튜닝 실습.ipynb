{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOe8kO10q3BVALaqWz/Drrn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"017f7a669d814110a847b11495f9dc6d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33ace584fcf94e84abdbf8c6b64bbe47","IPY_MODEL_35411f4062494e34bf911771b66c3a46","IPY_MODEL_3d4ddde63c0c4e3fa8d1a5fcb04a669a"],"layout":"IPY_MODEL_e5b9bbcb270a40ffbf4d94c88ed64480"}},"33ace584fcf94e84abdbf8c6b64bbe47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9f04034b229458caa0b7375f50a31b8","placeholder":"​","style":"IPY_MODEL_685210b9df0d49b684cd44bfee46e212","value":"Generating train split: "}},"35411f4062494e34bf911771b66c3a46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5e7f0515df54d7385c53653518591ba","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d7a5ff5609643c0bd8ace517968ee68","value":1}},"3d4ddde63c0c4e3fa8d1a5fcb04a669a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55f149cb2e42474eb16587db284f8739","placeholder":"​","style":"IPY_MODEL_0639a84deff848beaf131d21dc7a1ae6","value":" 31/0 [00:00&lt;00:00, 516.76 examples/s]"}},"e5b9bbcb270a40ffbf4d94c88ed64480":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9f04034b229458caa0b7375f50a31b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"685210b9df0d49b684cd44bfee46e212":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5e7f0515df54d7385c53653518591ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"0d7a5ff5609643c0bd8ace517968ee68":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55f149cb2e42474eb16587db284f8739":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0639a84deff848beaf131d21dc7a1ae6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49a389882e2942ba8b87ae6090970d1f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0056ab790c4f4943b5600620ddb0c11f","IPY_MODEL_82df1255212e4465a6ab3ba2e1b82c7c","IPY_MODEL_3e84cda267db429c8a7ab50b05692c63"],"layout":"IPY_MODEL_116ef09241a044c69b19309cc3e37683"}},"0056ab790c4f4943b5600620ddb0c11f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9905ae9ce5ab443dbefca7ae91fa5112","placeholder":"​","style":"IPY_MODEL_b60bd6b7252949c9adbbd729190c23f5","value":"Uploading the dataset shards: 100%"}},"82df1255212e4465a6ab3ba2e1b82c7c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79b418f7194543559021f699f9300511","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4589e1fe3dd04eefbb14f77efd411507","value":1}},"3e84cda267db429c8a7ab50b05692c63":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e397520e64a94bf8b4a2063acda10a46","placeholder":"​","style":"IPY_MODEL_e0a7c060dec14a409827fb429c77cc4c","value":" 1/1 [00:00&lt;00:00,  1.51it/s]"}},"116ef09241a044c69b19309cc3e37683":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9905ae9ce5ab443dbefca7ae91fa5112":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b60bd6b7252949c9adbbd729190c23f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79b418f7194543559021f699f9300511":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4589e1fe3dd04eefbb14f77efd411507":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e397520e64a94bf8b4a2063acda10a46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0a7c060dec14a409827fb429c77cc4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61cdfb7a6bc4435195b1015ed4a83bfb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91ce6fd56fab46b6bf4ed74f28ea9ffb","IPY_MODEL_f6f9170b8eb0489ba132cb79ad5260ac","IPY_MODEL_3d7147c8d44140b19eba0a59490ea554"],"layout":"IPY_MODEL_bb6b10f1c4dd45d38734068be9691eda"}},"91ce6fd56fab46b6bf4ed74f28ea9ffb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3de6fc68ceb74e998b03aab91bcbfc3e","placeholder":"​","style":"IPY_MODEL_cf31470f93ea406085e8d0bdde74ccde","value":"Creating parquet from Arrow format: 100%"}},"f6f9170b8eb0489ba132cb79ad5260ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc7e5728a1034528b7f232c91e1ba1c4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_395314ab07314ecfbac65a4b533a89ce","value":1}},"3d7147c8d44140b19eba0a59490ea554":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08883c20b71a47d88d93aa15fc6b6584","placeholder":"​","style":"IPY_MODEL_198a081d339c43d186d38ad879006c1d","value":" 1/1 [00:00&lt;00:00, 25.09ba/s]"}},"bb6b10f1c4dd45d38734068be9691eda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3de6fc68ceb74e998b03aab91bcbfc3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf31470f93ea406085e8d0bdde74ccde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc7e5728a1034528b7f232c91e1ba1c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"395314ab07314ecfbac65a4b533a89ce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08883c20b71a47d88d93aa15fc6b6584":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"198a081d339c43d186d38ad879006c1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"810bf2b0adb7426c992ca28611793b64":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_440b3ddc5c68458f9cdb608aa051b65b","IPY_MODEL_b5266ed4763f48419c0e4aa2c2f6aea3","IPY_MODEL_5589c9a3a6df427abce23c5193795aa6"],"layout":"IPY_MODEL_2e5c248e9d3646549317eaef7e571afe"}},"440b3ddc5c68458f9cdb608aa051b65b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ee7796962e8450fb8c96d5bc9cc16f1","placeholder":"​","style":"IPY_MODEL_86cc50947c9a41be8673366957b1c42f","value":"model.safetensors: 100%"}},"b5266ed4763f48419c0e4aa2c2f6aea3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46e8a34610e14e37bdd9325ff8ae8406","max":204989660,"min":0,"orientation":"horizontal","style":"IPY_MODEL_539c9b69d66e453eac8d2dde3fcffd2e","value":204989660}},"5589c9a3a6df427abce23c5193795aa6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3443b2fb1ba348b0828052182a721b02","placeholder":"​","style":"IPY_MODEL_8e023d5795c64b18be116e04d72cc55c","value":" 205M/205M [00:20&lt;00:00, 14.7MB/s]"}},"2e5c248e9d3646549317eaef7e571afe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ee7796962e8450fb8c96d5bc9cc16f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86cc50947c9a41be8673366957b1c42f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46e8a34610e14e37bdd9325ff8ae8406":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"539c9b69d66e453eac8d2dde3fcffd2e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3443b2fb1ba348b0828052182a721b02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e023d5795c64b18be116e04d72cc55c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76fe3b5135874e9787e7a75e4782325e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45f40256c4f24213818d9c5595afa7bd","IPY_MODEL_717a803478bf4de7bb44b88633ca3a99","IPY_MODEL_ad64e9c5dd4d4c86b908ae786a50a904"],"layout":"IPY_MODEL_d323d8dea6f04b188787e8cfdf88e293"}},"45f40256c4f24213818d9c5595afa7bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6db2cd070a6d46b4b2ebf02ff7ca2f2a","placeholder":"​","style":"IPY_MODEL_d268cad9b9c24fe89aaa74bb4811cbfb","value":"Generating train split: "}},"717a803478bf4de7bb44b88633ca3a99":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdcf9ec3d67549969d83bbfaa0b284f6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_62ccd31d129b485993ffde357b13b3a0","value":1}},"ad64e9c5dd4d4c86b908ae786a50a904":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c06713a08d7545c3ba9d329a93199ddc","placeholder":"​","style":"IPY_MODEL_c333de4f3472456d8f7466f41239183d","value":" 1195228/0 [00:10&lt;00:00, 115171.95 examples/s]"}},"d323d8dea6f04b188787e8cfdf88e293":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6db2cd070a6d46b4b2ebf02ff7ca2f2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d268cad9b9c24fe89aaa74bb4811cbfb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdcf9ec3d67549969d83bbfaa0b284f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"62ccd31d129b485993ffde357b13b3a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c06713a08d7545c3ba9d329a93199ddc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c333de4f3472456d8f7466f41239183d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4051ed738b434288b414af29eb72ef60":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ddf25cf40b9542d2957d3f8db4540b64","IPY_MODEL_fc42cd57b2f942a4a77fd31824959acd","IPY_MODEL_152908a0b9dd4a86b2b2f237afdbb84c"],"layout":"IPY_MODEL_4dd171bb02f14deda9a62697cde157a7"}},"ddf25cf40b9542d2957d3f8db4540b64":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d891c29fc77b43f7a0de9216e8e94051","placeholder":"​","style":"IPY_MODEL_8dd48904b3f94a129adb2309a56bd280","value":"Uploading the dataset shards: 100%"}},"fc42cd57b2f942a4a77fd31824959acd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cc6e642c6f44e7eb9b731a82d7fe6dd","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_474123f69e1d4470b67b0f3989e8d80b","value":1}},"152908a0b9dd4a86b2b2f237afdbb84c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16c99a801c7148fea5213ad98ce74d13","placeholder":"​","style":"IPY_MODEL_d6fc4ead6afa4640893e66af83589bfd","value":" 1/1 [00:21&lt;00:00, 21.93s/it]"}},"4dd171bb02f14deda9a62697cde157a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d891c29fc77b43f7a0de9216e8e94051":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dd48904b3f94a129adb2309a56bd280":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cc6e642c6f44e7eb9b731a82d7fe6dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"474123f69e1d4470b67b0f3989e8d80b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"16c99a801c7148fea5213ad98ce74d13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6fc4ead6afa4640893e66af83589bfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51ac1d786d50455880183bee2696505b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1a9a066a67ac4a03a3c8eaa91b840623","IPY_MODEL_f59bab071f9a4c3b83777bdfcffba753","IPY_MODEL_51ccd14654694f4794d62a06126bcc56"],"layout":"IPY_MODEL_00d99041c015448080f92806af81c53f"}},"1a9a066a67ac4a03a3c8eaa91b840623":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91b65a082d5f4d3bb6454f8454de4ef5","placeholder":"​","style":"IPY_MODEL_7ba45f88c062419bbf2a043d96e1c949","value":"Creating parquet from Arrow format: 100%"}},"f59bab071f9a4c3b83777bdfcffba753":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9f6ea5689894661ab4191accb012f5d","max":1196,"min":0,"orientation":"horizontal","style":"IPY_MODEL_598d637235254715b0fec3c8b575260a","value":1196}},"51ccd14654694f4794d62a06126bcc56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46e2465bf87d4779bb8e2a3432dc3d86","placeholder":"​","style":"IPY_MODEL_7b9211057de0481ba675512e11a6f6a0","value":" 1196/1196 [00:03&lt;00:00, 355.67ba/s]"}},"00d99041c015448080f92806af81c53f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91b65a082d5f4d3bb6454f8454de4ef5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ba45f88c062419bbf2a043d96e1c949":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9f6ea5689894661ab4191accb012f5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"598d637235254715b0fec3c8b575260a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46e2465bf87d4779bb8e2a3432dc3d86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b9211057de0481ba675512e11a6f6a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d4ed3b1c2ad471ab811a6cd8ba9647e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffbd401a83884dc4834376389809a4f6","IPY_MODEL_cbcea42809464a7a968065b46563c610","IPY_MODEL_093ec959e1a04396a16a68b857b2f3d1"],"layout":"IPY_MODEL_5531a8b3aae548b1a2bc82853b606389"}},"ffbd401a83884dc4834376389809a4f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abe3170e34df4f72bfc1d743ee202bd0","placeholder":"​","style":"IPY_MODEL_f68b2dcce08645c09133e0b6598eac14","value":"Generating train split: "}},"cbcea42809464a7a968065b46563c610":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_575bf6a064274d2e9710ff4f46903bbb","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a84e340ea0cf484a9c9bed749e3e97c0","value":1}},"093ec959e1a04396a16a68b857b2f3d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13b04b4e0a6f48a69189457d63e95d11","placeholder":"​","style":"IPY_MODEL_77acc2d58df449f3bb51071ebf8fb04b","value":" 64133/0 [03:52&lt;00:00, 341.59 examples/s]"}},"5531a8b3aae548b1a2bc82853b606389":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abe3170e34df4f72bfc1d743ee202bd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f68b2dcce08645c09133e0b6598eac14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"575bf6a064274d2e9710ff4f46903bbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a84e340ea0cf484a9c9bed749e3e97c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13b04b4e0a6f48a69189457d63e95d11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77acc2d58df449f3bb51071ebf8fb04b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["accelerate : PyTorch 모델의 학습 속도 향상과 추론 최적화를 위한 라이브러리\n","\n","peft : Parameter Efficient Fine Tunning의 약자로 대규모 언어 모델을 효율적으로 미세 조정할 수 있는 PEFT 기술 구현\n","\n","bitsandbytes : 모델 매개변수 양자화로 메모리 사용량 절감\n","\n","transfomers : 다양한 자연어 처리 모델을 쉽게 사용할 수 있는 API 제공\n","\n","\n","trl : Transformer Reinforcement Learning의 약자로 강화 학습 기반 언어 모델 미세 조정 기술 구현\n","\n","datassets : 자연어 처리 데이터셋 다운로드 및 전처리 지원\n"],"metadata":{"id":"HQsNDsOvuG3m"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fACndwkAsDKw","executionInfo":{"status":"ok","timestamp":1727842564622,"user_tz":-540,"elapsed":23416,"user":{"displayName":"우다원","userId":"06820525949954151974"}},"outputId":"0382b4b4-16f0-4412-fc99-52c5d213753d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate==0.29.3\n","  Using cached accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: peft==0.10.0 in /usr/local/lib/python3.10/dist-packages (0.10.0)\n","Requirement already satisfied: bitsandbytes==0.43.1 in /usr/local/lib/python3.10/dist-packages (0.43.1)\n","Collecting transformers==4.40.1\n","  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n","Requirement already satisfied: trl==0.8.6 in /usr/local/lib/python3.10/dist-packages (0.8.6)\n","Requirement already satisfied: datasets==2.19.0 in /usr/local/lib/python3.10/dist-packages (2.19.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (6.0.2)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (2.4.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (0.24.7)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (0.4.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.66.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.1) (3.16.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.1) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.1) (2.32.3)\n","Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.1)\n","  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl==0.8.6) (0.8.11)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0) (2024.3.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.19.0) (3.10.6)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.0) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.0) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.0) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.0) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.0) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.0) (1.12.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.19.0) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.29.3) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.1) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.1) (2024.8.30)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1.4)\n","Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.8.6) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.8.6) (13.8.1)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.8.6) (1.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.0) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.19.0) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.18.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.29.3) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\n","Using cached accelerate-0.29.3-py3-none-any.whl (297 kB)\n","Using cached transformers-4.40.1-py3-none-any.whl (9.0 MB)\n","Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","Installing collected packages: tokenizers, accelerate, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.20.0\n","    Uninstalling tokenizers-0.20.0:\n","      Successfully uninstalled tokenizers-0.20.0\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.34.2\n","    Uninstalling accelerate-0.34.2:\n","      Successfully uninstalled accelerate-0.34.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.45.1\n","    Uninstalling transformers-4.45.1:\n","      Successfully uninstalled transformers-4.45.1\n","Successfully installed accelerate-0.29.3 tokenizers-0.19.1 transformers-4.40.1\n"]}],"source":["pip install -U accelerate==0.29.3 peft==0.10.0 bitsandbytes==0.43.1 transformers==4.40.1 trl==0.8.6 datasets==2.19.0"]},{"cell_type":"code","source":["pip install -i https://pypi.org/simple/ bitsandbytes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O7ZWRW5Bh5SC","executionInfo":{"status":"ok","timestamp":1727842569496,"user_tz":-540,"elapsed":4876,"user":{"displayName":"우다원","userId":"06820525949954151974"}},"outputId":"e28a1f09-e073-479a-d068-d1d418ccf940"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple/\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install accelerate bitsandbytes\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6o25snW9idVY","executionInfo":{"status":"ok","timestamp":1727842575458,"user_tz":-540,"elapsed":5967,"user":{"displayName":"우다원","userId":"06820525949954151974"}},"outputId":"50b9b85b-0cd7-4386-f0cf-790038b49879"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.7)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}]},{"cell_type":"code","source":["import json\n","\n","# JSON 파일 읽기\n","with open('new_test.json', 'r', encoding='utf-8') as f:\n","    json_data = json.load(f)\n","    json_test = json_data['data']\n","\n","# 전처리된 데이터 저장\n","ko_en_list = []\n","for item in json_test:\n","    if 'ko' in item and 'en' in item:\n","        ko_en_list.append({\n","            \"ko\": item[\"ko\"],\n","            \"en\": item[\"en\"]\n","        })\n","\n","# 새로운 JSON 파일 생성\n","with open('ko_en_test.json', 'w', encoding='utf-8') as f:\n","    json.dump(ko_en_list, f, ensure_ascii=False, indent=4)  # \"data\" 키를 없앰\n","\n","import huggingface_hub\n","huggingface_hub.login('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')  # API 키 입력\n","\n","from datasets import Dataset\n","dataset = Dataset.from_json('ko_en_test.json')\n","dataset.push_to_hub('dawon62/test')\n","\n","# 데이터 로드 및 모델 설정\n","import os\n","import torch\n","from datasets import load_dataset\n","import bitsandbytes as bnb\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n",")\n","from peft import LoraConfig\n","from trl import SFTTrainer\n","\n","base_model = \"Helsinki-NLP/opus-mt-tc-big-en-ko\"  # 학습할 모델\n","hkcode_dataset = \"dawon62/test\"  # 데이터셋\n","new_model = \"dawon62/hkcode-mt-en-ko\"  # 새로운 모델\n","\n","# CUDA 환경 설정\n","if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n","    !pip install -qqq flash-attn\n","    attn_implementation = \"flash_attention_2\"\n","    torch_dtype = torch.bfloat16\n","else:\n","    attn_implementation = \"eager\"\n","    torch_dtype = torch.float16\n","\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch_dtype,\n","    bnb_4bit_use_double_quant=False,\n",")\n","\n","# 데이터셋 로드\n","dataset = load_dataset(hkcode_dataset, split=\"train\")\n","print(dataset)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    quantization_config=quant_config,\n","    device_map={\"\": 0}\n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    base_model,\n","    trust_remote_code=True\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","# PEFT 파라미터 설정\n","peft_params = LoraConfig(\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# TrainingArguments 설정\n","training_params = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=3,  # 에포크 수를 늘림\n","    per_device_train_batch_size=2,  # 배치 크기 증가\n","    gradient_accumulation_steps=1,\n","    optim=\"adamw_hf\",\n","    save_steps=50,\n","    logging_steps=50,\n","    learning_rate=2e-5,  # 학습률 조정\n","    weight_decay=0.01,\n","    fp16=True,  # fp16 사용\n","    bf16=False,\n","    max_grad_norm=1.0,  # Gradient Clipping\n","    max_steps=-1,\n","    warmup_ratio=0.1,  # Warmup 비율 조정\n","    group_by_length=True,\n","    lr_scheduler_type=\"linear\",  # 다른 스케줄러 사용\n","    report_to=\"tensorboard\",\n",")\n","\n","# SFTTrainer 설정\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_params,\n","    dataset_text_field=\"ko\",  # \"ko\" 또는 \"en\" 필드로 수정\n","    max_seq_length=128,  # 적절한 길이로 설정\n","    tokenizer=tokenizer,\n","    args=training_params,\n","    packing=True,\n",")\n","\n","# 트레이너 실행\n","trainer.train()\n","\n","model.push_to_hub(\"dawon62/hkcode-mt-en-ko\")\n","tokenizer.push_to_hub(\"dawon62/hkcode-mt-en-ko\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":805,"referenced_widgets":["017f7a669d814110a847b11495f9dc6d","33ace584fcf94e84abdbf8c6b64bbe47","35411f4062494e34bf911771b66c3a46","3d4ddde63c0c4e3fa8d1a5fcb04a669a","e5b9bbcb270a40ffbf4d94c88ed64480","b9f04034b229458caa0b7375f50a31b8","685210b9df0d49b684cd44bfee46e212","d5e7f0515df54d7385c53653518591ba","0d7a5ff5609643c0bd8ace517968ee68","55f149cb2e42474eb16587db284f8739","0639a84deff848beaf131d21dc7a1ae6","49a389882e2942ba8b87ae6090970d1f","0056ab790c4f4943b5600620ddb0c11f","82df1255212e4465a6ab3ba2e1b82c7c","3e84cda267db429c8a7ab50b05692c63","116ef09241a044c69b19309cc3e37683","9905ae9ce5ab443dbefca7ae91fa5112","b60bd6b7252949c9adbbd729190c23f5","79b418f7194543559021f699f9300511","4589e1fe3dd04eefbb14f77efd411507","e397520e64a94bf8b4a2063acda10a46","e0a7c060dec14a409827fb429c77cc4c","61cdfb7a6bc4435195b1015ed4a83bfb","91ce6fd56fab46b6bf4ed74f28ea9ffb","f6f9170b8eb0489ba132cb79ad5260ac","3d7147c8d44140b19eba0a59490ea554","bb6b10f1c4dd45d38734068be9691eda","3de6fc68ceb74e998b03aab91bcbfc3e","cf31470f93ea406085e8d0bdde74ccde","dc7e5728a1034528b7f232c91e1ba1c4","395314ab07314ecfbac65a4b533a89ce","08883c20b71a47d88d93aa15fc6b6584","198a081d339c43d186d38ad879006c1d","810bf2b0adb7426c992ca28611793b64","440b3ddc5c68458f9cdb608aa051b65b","b5266ed4763f48419c0e4aa2c2f6aea3","5589c9a3a6df427abce23c5193795aa6","2e5c248e9d3646549317eaef7e571afe","5ee7796962e8450fb8c96d5bc9cc16f1","86cc50947c9a41be8673366957b1c42f","46e8a34610e14e37bdd9325ff8ae8406","539c9b69d66e453eac8d2dde3fcffd2e","3443b2fb1ba348b0828052182a721b02","8e023d5795c64b18be116e04d72cc55c"]},"id":"6hCvFoUo2y0t","executionInfo":{"status":"ok","timestamp":1727842637405,"user_tz":-540,"elapsed":61950,"user":{"displayName":"우다원","userId":"06820525949954151974"}},"outputId":"3aae3b6b-3162-43b4-f8d8-fc87d8e727c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"017f7a669d814110a847b11495f9dc6d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a389882e2942ba8b87ae6090970d1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61cdfb7a6bc4435195b1015ed4a83bfb"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['ko', 'en'],\n","    num_rows: 31\n","})\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of MarianForCausalLM were not initialized from the model checkpoint at Helsinki-NLP/opus-mt-tc-big-en-ko and are newly initialized: ['lm_head.weight', 'model.decoder.embed_positions.weight', 'model.decoder.embed_tokens.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n","/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:469: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15/15 00:00, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[32000]], 'forced_eos_token_id': 2}\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/205M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"810bf2b0adb7426c992ca28611793b64"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"output_type":"execute_result","data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/dawon62/hkcode-mt-en-ko/commit/2e17da287d387415ce63986d048d52c21c6ec211', commit_message='Upload tokenizer', commit_description='', oid='2e17da287d387415ce63986d048d52c21c6ec211', pr_url=None, pr_revision=None, pr_num=None)"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["!pip install --upgrade transformers torch accelerate\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":971},"id":"KhAz-97Wb35l","executionInfo":{"status":"ok","timestamp":1727842658796,"user_tz":-540,"elapsed":21394,"user":{"displayName":"우다원","userId":"06820525949954151974"}},"outputId":"811714e2-da09-436d-ac1c-3a244350c724"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n","Collecting transformers\n","  Using cached transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n","Collecting accelerate\n","  Using cached accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Collecting tokenizers<0.21,>=0.20 (from transformers)\n","  Using cached tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.3.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Using cached transformers-4.45.1-py3-none-any.whl (9.9 MB)\n","Using cached accelerate-0.34.2-py3-none-any.whl (324 kB)\n","Using cached tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n","Installing collected packages: tokenizers, accelerate, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.19.1\n","    Uninstalling tokenizers-0.19.1:\n","      Successfully uninstalled tokenizers-0.19.1\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.29.3\n","    Uninstalling accelerate-0.29.3:\n","      Successfully uninstalled accelerate-0.29.3\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.40.1\n","    Uninstalling transformers-4.40.1:\n","      Successfully uninstalled transformers-4.40.1\n","Successfully installed accelerate-0.34.2 tokenizers-0.20.0 transformers-4.45.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["accelerate","transformers"]},"id":"1c28c6eef4e04f4d86b364f7e6ccd7f2"}},"metadata":{}}]},{"cell_type":"code","source":["from transformers import MarianMTModel, MarianTokenizer\n","from huggingface_hub import login\n","\n","# Hugging Face 로그인\n","api_token = \"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\"  # 여기에 API 토큰을 입력하세요.\n","login(token=api_token)\n","\n","# 모델 이름\n","model_name = \"dawon62/hkcode-mt-en-ko\"\n","\n","# 모델과 토크나이저 로드 (CPU에서 로드)\n","tokenizer = MarianTokenizer.from_pretrained(model_name)\n","model = MarianMTModel.from_pretrained(model_name, device_map=\"cpu\")  # device_map을 \"cpu\"로 설정\n","\n","# 사용 예시 (텍스트 번역 등)\n","text = \"안녕하세요, 저는 AI 모델입니다.\"\n","inputs = tokenizer(text, return_tensors=\"pt\")\n","\n","# 모델을 사용하여 번역\n","with torch.no_grad():\n","    outputs = model.generate(**inputs)\n","\n","translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(translated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535},"id":"WpczyEcECHHE","executionInfo":{"status":"error","timestamp":1727843846828,"user_tz":-540,"elapsed":18128,"user":{"displayName":"우다원","userId":"06820525949954151974"}},"outputId":"fa039e4a-f17e-4493-9b2f-6b9e81f29f47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"output_type":"stream","name":"stderr","text":["Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","Some weights of MarianMTModel were not initialized from the model checkpoint at dawon62/hkcode-mt-en-ko and are newly initialized: ['model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"error","ename":"ValueError","evalue":"weight is on the meta device, we need a `value` to put in on cpu.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-ba1d3fa35211>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 모델과 토크나이저 로드 (CPU에서 로드)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMarianTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMarianMTModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# device_map을 \"cpu\"로 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 사용 예시 (텍스트 번역 등)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4090\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4091\u001b[0;31m                 \u001b[0mdispatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdevice_map_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py\u001b[0m in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0;31m# as we have no guarantee that safetensors' `file.get_tensor()` will always give the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         attach_align_device_hook_on_blocks(\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mexecution_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mtied_params_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtied_params_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0madd_hook_to_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m         \u001b[0mattach_execution_device_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_device\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtied_params_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtied_params_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexecution_device\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moffload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36madd_hook_to_module\u001b[0;34m(module, hook, append)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36minit_hook\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_module_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_submodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                 \u001b[0mset_module_tensor_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtied_params_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtied_params_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             self.original_devices = {\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{tensor_name} is on the meta device, we need a `value` to put in on {device}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtensor_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: weight is on the meta device, we need a `value` to put in on cpu."]}]},{"cell_type":"code","source":["import json\n","import huggingface_hub\n","from datasets import Dataset\n","import os\n","import torch\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    TrainingArguments,\n",")\n","from peft import LoraConfig\n","from trl import SFTTrainer\n","\n","# JSON 파일 읽기\n","with open('1113_tech_train_set_1195228.json', 'r', encoding='utf-8') as f:\n","    json_data = json.load(f)\n","    json_test = json_data['data']\n","\n","# 전처리된 데이터 저장\n","ko_en_list = []\n","for item in json_test:\n","    if 'ko' in item and 'en' in item:\n","        ko_en_list.append({\n","            \"ko\": item[\"ko\"],\n","            \"en\": item[\"en\"]\n","        })\n","\n","# 새로운 JSON 파일 생성\n","with open('ko_en_test.json', 'w', encoding='utf-8') as f:\n","    json.dump(ko_en_list, f, ensure_ascii=False, indent=4)  # \"data\" 키를 없앰\n","\n","# Hugging Face에 로그인\n","huggingface_hub.login('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')  # API 키 입력\n","\n","# 데이터셋 로드\n","dataset = Dataset.from_json('ko_en_test.json')\n","dataset.push_to_hub('dawon62/test')\n","\n","# 데이터 로드 및 모델 설정\n","base_model = \"Helsinki-NLP/opus-mt-tc-big-en-ko\"  # 학습할 모델\n","\n","# 모델과 토크나이저 설정\n","model = AutoModelForCausalLM.from_pretrained(base_model)\n","tokenizer = AutoTokenizer.from_pretrained(base_model)\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","# PEFT 파라미터 설정\n","peft_params = LoraConfig(\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# TrainingArguments 설정\n","training_params = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=3,  # 에포크 수\n","    per_device_train_batch_size=2,  # 배치 크기\n","    gradient_accumulation_steps=1,\n","    optim=\"adamw_hf\",\n","    save_steps=50,\n","    logging_steps=50,\n","    learning_rate=2e-5,  # 학습률\n","    weight_decay=0.01,\n","    bf16=True,  # bf16 사용\n","    fp16=False,  # fp16 비활성화\n","    max_grad_norm=1.0,  # Gradient Clipping\n","    max_steps=-1,\n","    warmup_ratio=0.1,  # Warmup 비율\n","    group_by_length=True,\n","    lr_scheduler_type=\"linear\",  # 학습률 스케줄러\n","    report_to=\"tensorboard\",\n",")\n","\n","# SFTTrainer 설정\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_params,\n","    dataset_text_field=\"en\",  # 입력 텍스트 필드\n","    tokenizer=tokenizer,\n","    args=training_params,\n","    packing=True,\n",")\n","\n","# 트레이너 실행\n","trainer.train()\n","\n","# 훈련된 모델을 Hugging Face에 업로드\n","model.push_to_hub(\"dawon62/hkcode-mt-en-ko\")\n","tokenizer.push_to_hub(\"dawon62/hkcode-mt-en-ko\")\n","\n","# 모델 로드 및 테스트\n","loaded_model = AutoModelForCausalLM.from_pretrained('dawon62/hkcode-mt-en-ko')\n","loaded_tokenizer = AutoTokenizer.from_pretrained('dawon62/hkcode-mt-en-ko')\n","\n","# 테스트 함수\n","def translate(text):\n","    inputs = loaded_tokenizer(text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = loaded_model.generate(**inputs)\n","    return loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# 번역 테스트\n","sample_text = \"hello, how are you?\"\n","translated_text = translate(sample_text)\n","print(f\"원문: {sample_text}\")\n","print(f\"번역: {translated_text}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["76fe3b5135874e9787e7a75e4782325e","45f40256c4f24213818d9c5595afa7bd","717a803478bf4de7bb44b88633ca3a99","ad64e9c5dd4d4c86b908ae786a50a904","d323d8dea6f04b188787e8cfdf88e293","6db2cd070a6d46b4b2ebf02ff7ca2f2a","d268cad9b9c24fe89aaa74bb4811cbfb","fdcf9ec3d67549969d83bbfaa0b284f6","62ccd31d129b485993ffde357b13b3a0","c06713a08d7545c3ba9d329a93199ddc","c333de4f3472456d8f7466f41239183d","4051ed738b434288b414af29eb72ef60","ddf25cf40b9542d2957d3f8db4540b64","fc42cd57b2f942a4a77fd31824959acd","152908a0b9dd4a86b2b2f237afdbb84c","4dd171bb02f14deda9a62697cde157a7","d891c29fc77b43f7a0de9216e8e94051","8dd48904b3f94a129adb2309a56bd280","2cc6e642c6f44e7eb9b731a82d7fe6dd","474123f69e1d4470b67b0f3989e8d80b","16c99a801c7148fea5213ad98ce74d13","d6fc4ead6afa4640893e66af83589bfd","51ac1d786d50455880183bee2696505b","1a9a066a67ac4a03a3c8eaa91b840623","f59bab071f9a4c3b83777bdfcffba753","51ccd14654694f4794d62a06126bcc56","00d99041c015448080f92806af81c53f","91b65a082d5f4d3bb6454f8454de4ef5","7ba45f88c062419bbf2a043d96e1c949","c9f6ea5689894661ab4191accb012f5d","598d637235254715b0fec3c8b575260a","46e2465bf87d4779bb8e2a3432dc3d86","7b9211057de0481ba675512e11a6f6a0","0d4ed3b1c2ad471ab811a6cd8ba9647e","ffbd401a83884dc4834376389809a4f6","cbcea42809464a7a968065b46563c610","093ec959e1a04396a16a68b857b2f3d1","5531a8b3aae548b1a2bc82853b606389","abe3170e34df4f72bfc1d743ee202bd0","f68b2dcce08645c09133e0b6598eac14","575bf6a064274d2e9710ff4f46903bbb","a84e340ea0cf484a9c9bed749e3e97c0","13b04b4e0a6f48a69189457d63e95d11","77acc2d58df449f3bb51071ebf8fb04b"]},"id":"OXqft8f8iPTQ","outputId":"217d2923-e4f1-4949-8bc4-ec3d05f808c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76fe3b5135874e9787e7a75e4782325e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4051ed738b434288b414af29eb72ef60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Creating parquet from Arrow format:   0%|          | 0/1196 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51ac1d786d50455880183bee2696505b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of MarianForCausalLM were not initialized from the model checkpoint at Helsinki-NLP/opus-mt-tc-big-en-ko and are newly initialized: ['lm_head.weight', 'model.decoder.embed_positions.weight', 'model.decoder.embed_tokens.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 512\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d4ed3b1c2ad471ab811a6cd8ba9647e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='15905' max='96201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15905/96201 1:03:10 < 5:18:59, 4.20 it/s, Epoch 0.50/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>50</td>\n","      <td>5.240000</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>1950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>5950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>6950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>7950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>8950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>9950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>10950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>11950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>12950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>13950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14900</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>14950</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15050</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15150</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15200</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15250</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15300</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15350</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15400</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15450</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15550</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15650</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15700</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15750</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15850</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>15900</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["import json\n","import huggingface_hub\n","from datasets import Dataset\n","from datasets import load_dataset\n","import os\n","import torch\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    TrainingArguments,\n",")\n","from peft import LoraConfig\n","from trl import SFTTrainer\n","\n","# JSON 파일 읽기\n","with open('1113_tech_train_set_1195228.json', 'r', encoding='utf-8') as f:\n","    json_data = json.load(f)\n","    json_test = json_data['data']\n","\n","# 전처리된 데이터 저장\n","ko_en_list = []\n","for item in json_test:\n","    if 'ko' in item and 'en' in item:\n","        ko_en_list.append({\n","            \"ko\": item[\"ko\"],\n","            \"en\": item[\"en\"]\n","        })\n","\n","# 새로운 JSON 파일 생성\n","with open('ko_en_test.json', 'w', encoding='utf-8') as f:\n","    json.dump(ko_en_list, f, ensure_ascii=False, indent=4)  # \"data\" 키를 없앰\n","\n","# Hugging Face에 로그인\n","huggingface_hub.login('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')  # API 키 입력\n","\n","# 데이터셋 로드\n","dataset = Dataset.from_json('ko_en_test.json')\n","dataset.push_to_hub('dawon62/test')\n","hkcode_dataset = \"dawon62/test\"  # 데이터셋\n","dataset = load_dataset(hkcode_dataset, split=\"train\")\n","\n","# 데이터 로드 및 모델 설정\n","base_model = \"Helsinki-NLP/opus-mt-tc-big-en-ko\"  # 학습할 모델\n","\n","# 모델과 토크나이저 설정\n","model = AutoModelForCausalLM.from_pretrained(base_model)\n","tokenizer = AutoTokenizer.from_pretrained(base_model)\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","# PEFT 파라미터 설정\n","peft_params = LoraConfig(\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# TrainingArguments 설정\n","training_params = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=3,  # 에포크 수\n","    per_device_train_batch_size=2,  # 배치 크기\n","    gradient_accumulation_steps=1,\n","    optim=\"adamw_hf\",\n","    save_steps=50,\n","    logging_steps=50,\n","    learning_rate=2e-5,  # 학습률\n","    weight_decay=0.01,\n","    bf16=True,  # bf16 사용\n","    fp16=False,  # fp16 비활성화\n","    max_grad_norm=1.0,  # Gradient Clipping\n","    max_steps=-1,\n","    warmup_ratio=0.1,  # Warmup 비율\n","    group_by_length=True,\n","    lr_scheduler_type=\"linear\",  # 학습률 스케줄러\n","    report_to=\"tensorboard\",\n",")\n","\n","# SFTTrainer 설정\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_params,\n","    dataset_text_field=\"en\",  # 입력 텍스트 필드\n","    tokenizer=tokenizer,\n","    args=training_params,\n","    packing=False,\n",")\n","\n","# 트레이너 실행\n","trainer.train()\n","\n","# 훈련된 모델을 Hugging Face에 업로드\n","model.push_to_hub(\"dawon62/hkcode-mt-en-ko\")\n","tokenizer.push_to_hub(\"dawon62/hkcode-mt-en-ko\")\n","\n","# 모델 로드 및 테스트\n","loaded_model = AutoModelForCausalLM.from_pretrained('dawon62/hkcode-mt-en-ko')\n","loaded_tokenizer = AutoTokenizer.from_pretrained('dawon62/hkcode-mt-en-ko')\n","\n","# 테스트 함수\n","def translate(text):\n","    inputs = loaded_tokenizer(text, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = loaded_model.generate(**inputs)\n","    return loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# 번역 테스트\n","sample_text = \"hello, how are you?\"\n","translated_text = translate(sample_text)\n","print(f\"원문: {sample_text}\")\n","print(f\"번역: {translated_text}\")\n"],"metadata":{"id":"4RgcOpFZGG_j"},"execution_count":null,"outputs":[]}]}